{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skincancer HAM-dataset using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Standardimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "homePath = os.path.dirname(\"/home/johan/Dropbox/coding/\")\n",
    "# print(homePath) ---> /home/johan/Dropbox/coding\n",
    "\n",
    "basePath = homePath + \"/\" +\"ml/Datasets/skin-cancer-mnist-ham10000/images_in_one\"\n",
    "# print(basePath) ---> /home/johan/Dropbox/coding/ml/Datasets/skin-cancer-mnist-ham10000/\n",
    "\n",
    "imageFolder = \"images_per_label_splitted_sets\"\n",
    "\n",
    "metadataSkincancerFilename =  \"../../../ml/Datasets/skin-cancer-mnist-ham10000/csv/HAM10000_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../ml/Datasets/skin-cancer-mnist-ham10000/csv/HAM10000_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "print(metadataSkincancerFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importera metadatan i en Pandas DataFrame:\n",
    "\n",
    "skincancer_df = pd.read_csv(metadataSkincancerFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lesion_id        0\n",
       "image_id         0\n",
       "dx               0\n",
       "dx_type          0\n",
       "age             57\n",
       "sex              0\n",
       "localization     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kolla om vi har några noll-värden i vår Dataframe:\n",
    "\n",
    "skincancer_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enligt ovan är det bara i \"age\"-kolumnen som vi har noll-värden. \n",
    "# Dessa fyller vi ut genom att beräkna medevärdet:\n",
    "\n",
    "skincancer_df['age'].fillna((skincancer_df['age'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = skincancer_df['dx']\n",
    "num_classes = len(labels.unique()) # --> 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 0, 0, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "enc_labels = label_encoder.fit_transform(np_labels)\n",
    "enc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2,  ..., 0, 0, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_labels = torch.from_numpy(enc_labels)\n",
    "torch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>age</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>dx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>80.0</td>\n",
       "      <td>histo</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>80.0</td>\n",
       "      <td>histo</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>80.0</td>\n",
       "      <td>histo</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   age dx_type   sex localization   dx\n",
       "0  HAM_0000118  ISIC_0027419  80.0   histo  male        scalp  bkl\n",
       "1  HAM_0000118  ISIC_0025030  80.0   histo  male        scalp  bkl\n",
       "2  HAM_0002730  ISIC_0026769  80.0   histo  male        scalp  bkl"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Innan vi fortsätter måste vi göra om \n",
    "# text-datan till numeriska features.\n",
    "\n",
    "# 1. Flytta labels sist i df\n",
    "# 2. Flytta age till efter image_id\n",
    "# 3. Alla categorical features emellan.\n",
    "\n",
    "skincancer_df = skincancer_df[['lesion_id', 'image_id', 'age', 'dx_type', 'sex', 'localization', 'dx']]\n",
    "skincancer_df.head(n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "histo        5340\n",
       "follow_up    3704\n",
       "consensus     902\n",
       "confocal       69\n",
       "Name: dx_type, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dxtype_feat_df = skincancer_df.iloc[:, 3]\n",
    "dxtype_feat_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male       5406\n",
       "female     4552\n",
       "unknown      57\n",
       "Name: sex, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sex_feat_df = skincancer_df.iloc[:, 4]\n",
    "sex_feat_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "histo        5340\n",
       "follow_up    3704\n",
       "consensus     902\n",
       "confocal       69\n",
       "Name: dx_type, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_feat_df = skincancer_df.iloc[:, 3]\n",
    "loc_feat_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nv       6705\n",
       "mel      1113\n",
       "bkl      1099\n",
       "bcc       514\n",
       "akiec     327\n",
       "vasc      142\n",
       "df        115\n",
       "Name: dx, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df = skincancer_df.iloc[:, 6]\n",
    "label_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  69.,    0.,    0.,  902.,    0.,    0., 3704.,    0.,    0.,\n",
       "        5340.]),\n",
       " array([0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "dxtype_feat = le.fit_transform(dxtype_feat_df)\n",
    "plt.hist(dxtype_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  69.,    0.,    0.,  902.,    0.,    0., 3704.,    0.,    0.,\n",
       "        5340.]),\n",
       " array([0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "loc_feat = le.fit_transform(loc_feat_df)\n",
    "plt.hist(loc_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4552.,    0.,    0.,    0.,    0., 5406.,    0.,    0.,    0.,\n",
       "          57.]),\n",
       " array([0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8, 2. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAQgklEQVR4nO3cb8zd5V3H8fdnLUPchgNbsGnrikkfCMT9oal1GLMNlTqcxUSSLjoaQ9JIMNkSoyl7sMUHTdgTs5AIppkLJW4jTbZJs425ptuy6BjsZrKVwpDbgdCU0I6pAzWY4tcH5yI73pze97nb+5xDvd6v5OT8zvdc1/l9z8nVz/3r7/xJVSFJ6sPrZt2AJGl6DH1J6oihL0kdMfQlqSOGviR1ZPWsG1jKmjVratOmTbNuQ5LOKQ899NAPq2rtwvprPvQ3bdrE3NzcrNuQpHNKkn8ZVff0jiR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdeQ1/41c6bVq054vzmzfT9123cz2rXObR/qS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHxgr9JE8lOZLk4SRzrXZxkkNJnmjXFw2NvzXJfJLHk1w7VL+qPc58ktuTZOWfkiTpdJZzpP/uqnpbVW1pt/cAh6tqM3C43SbJ5cBO4ApgO3BHklVtzp3AbmBzu2w/+6cgSRrX2Zze2QHsb9v7geuH6vdU1UtV9SQwD2xNsg64sKrur6oC7h6aI0magnFDv4CvJHkoye5Wu7SqngVo15e0+nrgmaG5x1ptfdteWH+VJLuTzCWZO3ny5JgtSpKWMu7v6V9dVceTXAIcSvL9RcaOOk9fi9RfXazaB+wD2LJly8gxkqTlG+tIv6qOt+sTwOeBrcBz7ZQN7fpEG34M2Dg0fQNwvNU3jKhLkqZkydBP8oYkb3plG/hN4BHgILCrDdsF3Nu2DwI7k5yf5DIGb9g+2E4BvZBkW/vUzo1DcyRJUzDO6Z1Lgc+3T1euBj5dVV9O8m3gQJKbgKeBGwCq6miSA8CjwCnglqp6uT3WzcBdwAXAfe0iSZqSJUO/qn4AvHVE/XngmtPM2QvsHVGfA65cfpuSpJXgN3IlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOjPt7+uekTXu+OJP9PnXbdTPZryQtxSN9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6sjYoZ9kVZJ/TPKFdvviJIeSPNGuLxoae2uS+SSPJ7l2qH5VkiPtvtuTZGWfjiRpMcs50v8g8NjQ7T3A4araDBxut0lyObATuALYDtyRZFWbcyewG9jcLtvPqntJ0rKMFfpJNgDXAZ8YKu8A9rft/cD1Q/V7quqlqnoSmAe2JlkHXFhV91dVAXcPzZEkTcG4R/ofB/4M+J+h2qVV9SxAu76k1dcDzwyNO9Zq69v2wrokaUqWDP0kvw2cqKqHxnzMUefpa5H6qH3uTjKXZO7kyZNj7laStJRxjvSvBn4nyVPAPcB7kvwN8Fw7ZUO7PtHGHwM2Ds3fABxv9Q0j6q9SVfuqaktVbVm7du0yno4kaTFLhn5V3VpVG6pqE4M3aL9aVX8AHAR2tWG7gHvb9kFgZ5Lzk1zG4A3bB9spoBeSbGuf2rlxaI4kaQpWn8Xc24ADSW4CngZuAKiqo0kOAI8Cp4BbqurlNudm4C7gAuC+dpEkTcmyQr+qvg58vW0/D1xzmnF7gb0j6nPAlcttUpK0MvxGriR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerIkqGf5KeSPJjku0mOJvnzVr84yaEkT7Tri4bm3JpkPsnjSa4dql+V5Ei77/YkmczTkiSNMs6R/kvAe6rqrcDbgO1JtgF7gMNVtRk43G6T5HJgJ3AFsB24I8mq9lh3AruBze2yfQWfiyRpCUuGfg282G6e1y4F7AD2t/p+4Pq2vQO4p6peqqongXlga5J1wIVVdX9VFXD30BxJ0hSMdU4/yaokDwMngENV9QBwaVU9C9CuL2nD1wPPDE0/1mrr2/bC+qj97U4yl2Tu5MmTy3k+kqRFjBX6VfVyVb0N2MDgqP3KRYaPOk9fi9RH7W9fVW2pqi1r164dp0VJ0hiW9emdqvo34OsMzsU/107Z0K5PtGHHgI1D0zYAx1t9w4i6JGlKxvn0ztokb27bFwC/DnwfOAjsasN2Afe27YPAziTnJ7mMwRu2D7ZTQC8k2dY+tXPj0BxJ0hSsHmPMOmB/+wTO64ADVfWFJPcDB5LcBDwN3ABQVUeTHAAeBU4Bt1TVy+2xbgbuAi4A7msXSdKULBn6VfU94O0j6s8D15xmzl5g74j6HLDY+wGSpAnyG7mS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR1ZMvSTbEzytSSPJTma5IOtfnGSQ0meaNcXDc25Ncl8kseTXDtUvyrJkXbf7UkymaclSRplnCP9U8CfVNUvAtuAW5JcDuwBDlfVZuBwu027bydwBbAduCPJqvZYdwK7gc3tsn0Fn4skaQlLhn5VPVtV32nbLwCPAeuBHcD+Nmw/cH3b3gHcU1UvVdWTwDywNck64MKqur+qCrh7aI4kaQqWdU4/ySbg7cADwKVV9SwM/jAAl7Rh64FnhqYda7X1bXthfdR+dieZSzJ38uTJ5bQoSVrE2KGf5I3AZ4EPVdWPFxs6olaL1F9drNpXVVuqasvatWvHbVGStISxQj/JeQwC/1NV9blWfq6dsqFdn2j1Y8DGoekbgOOtvmFEXZI0JeN8eifAXwOPVdVfDN11ENjVtncB9w7VdyY5P8llDN6wfbCdAnohybb2mDcOzZEkTcHqMcZcDXwAOJLk4Vb7MHAbcCDJTcDTwA0AVXU0yQHgUQaf/Lmlql5u824G7gIuAO5rF0nSlCwZ+lX194w+Hw9wzWnm7AX2jqjPAVcup0FJ0srxG7mS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR1ZMvSTfDLJiSSPDNUuTnIoyRPt+qKh+25NMp/k8STXDtWvSnKk3Xd7kqz805EkLWacI/27gO0LanuAw1W1GTjcbpPkcmAncEWbc0eSVW3OncBuYHO7LHxMSdKELRn6VfUN4EcLyjuA/W17P3D9UP2eqnqpqp4E5oGtSdYBF1bV/VVVwN1DcyRJU3Km5/QvrapnAdr1Ja2+HnhmaNyxVlvfthfWR0qyO8lckrmTJ0+eYYuSpIVW+o3cUefpa5H6SFW1r6q2VNWWtWvXrlhzktS7Mw3959opG9r1iVY/BmwcGrcBON7qG0bUJUlTdKahfxDY1bZ3AfcO1XcmOT/JZQzesH2wnQJ6Icm29qmdG4fmSJKmZPVSA5J8BngXsCbJMeCjwG3AgSQ3AU8DNwBU1dEkB4BHgVPALVX1cnuomxl8EugC4L52kSRN0ZKhX1XvP81d15xm/F5g74j6HHDlsrqTJK0ov5ErSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOjL10E+yPcnjSeaT7Jn2/iWpZ6unubMkq4C/BH4DOAZ8O8nBqnp0mn1I0rg27fniTPb71G3XTeRxp32kvxWYr6ofVNV/A/cAO6bcgyR1a6pH+sB64Jmh28eAX144KMluYHe7+WKSx89wf2uAH57h3DOWjy05ZCZ9jcG+lmdmfS2xxny9luc12Vc+dtZ9vWVUcdqhnxG1elWhah+w76x3lsxV1ZazfZyVZl/LY1/LY1/L01tf0z69cwzYOHR7A3B8yj1IUremHfrfBjYnuSzJ64GdwMEp9yBJ3Zrq6Z2qOpXkj4G/A1YBn6yqoxPc5VmfIpoQ+1oe+1oe+1qervpK1atOqUuS/p/yG7mS1BFDX5I6ck6G/lI/5ZCB29v930vyjnHnTriv32/9fC/JN5O8dei+p5IcSfJwkrkp9/WuJP/e9v1wko+MO3fCff3pUE+PJHk5ycXtvkm+Xp9MciLJI6e5f1bra6m+ZrW+luprVutrqb5mtb42JvlakseSHE3ywRFjJrfGquqcujB4A/ifgV8AXg98F7h8wZj3Avcx+F7ANuCBcedOuK93Ahe17d96pa92+ylgzYxer3cBXziTuZPsa8H49wFfnfTr1R7714B3AI+c5v6pr68x+5r6+hqzr6mvr3H6muH6Wge8o22/CfinaWbYuXikP85POewA7q6BbwFvTrJuzLkT66uqvllV/9pufovB9xQm7Wye80xfrwXeD3xmhfa9qKr6BvCjRYbMYn0t2deM1tc4r9fpzPT1WmCa6+vZqvpO234BeIzBrxUMm9gaOxdDf9RPOSx8wU43Zpy5k+xr2E0M/pK/ooCvJHkog5+hWCnj9vUrSb6b5L4kVyxz7iT7IslPA9uBzw6VJ/V6jWMW62u5prW+xjXt9TW2Wa6vJJuAtwMPLLhrYmts2j/DsBLG+SmH040Z62cgztDYj53k3Qz+Uf7qUPnqqjqe5BLgUJLvtyOVafT1HeAtVfVikvcCfwtsHnPuJPt6xfuAf6iq4aO2Sb1e45jF+hrblNfXOGaxvpZjJusryRsZ/KH5UFX9eOHdI6asyBo7F4/0x/kph9ONmeTPQIz12El+CfgEsKOqnn+lXlXH2/UJ4PMM/hs3lb6q6sdV9WLb/hJwXpI148ydZF9DdrLgv94TfL3GMYv1NZYZrK8lzWh9LcfU11eS8xgE/qeq6nMjhkxujU3ijYpJXhj87+QHwGX85I2MKxaMuY7/+ybIg+POnXBfPw/MA+9cUH8D8Kah7W8C26fY18/xky/qbQWebq/dTF+vNu5nGJyXfcM0Xq+hfWzi9G9MTn19jdnX1NfXmH1NfX2N09es1ld77ncDH19kzMTW2Dl3eqdO81MOSf6o3f9XwJcYvPs9D/wn8IeLzZ1iXx8Bfha4IwnAqRr8it6lwOdbbTXw6ar68hT7+j3g5iSngP8CdtZghc369QL4XeArVfUfQ9Mn9noBJPkMg0+crElyDPgocN5QX1NfX2P2NfX1NWZfU19fY/YFM1hfwNXAB4AjSR5utQ8z+KM98TXmzzBIUkfOxXP6kqQzZOhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjvwv+LldV3tyNk8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "sex_feat = le.fit_transform(sex_feat_df)\n",
    "plt.hist(sex_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 327.,  514.,    0., 1099.,    0.,  115., 1113.,    0., 6705.,\n",
       "         142.]),\n",
       " array([0. , 0.6, 1.2, 1.8, 2.4, 3. , 3.6, 4.2, 4.8, 5.4, 6. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(label_df)\n",
    "plt.hist(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>age</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>dx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>80.0</td>\n",
       "      <td>histo</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>80.0</td>\n",
       "      <td>histo</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>80.0</td>\n",
       "      <td>histo</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>80.0</td>\n",
       "      <td>histo</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>75.0</td>\n",
       "      <td>histo</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "      <td>bkl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   age dx_type   sex localization   dx\n",
       "0  HAM_0000118  ISIC_0027419  80.0   histo  male        scalp  bkl\n",
       "1  HAM_0000118  ISIC_0025030  80.0   histo  male        scalp  bkl\n",
       "2  HAM_0002730  ISIC_0026769  80.0   histo  male        scalp  bkl\n",
       "3  HAM_0002730  ISIC_0025661  80.0   histo  male        scalp  bkl\n",
       "4  HAM_0001466  ISIC_0031633  75.0   histo  male          ear  bkl"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skincancer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_skincancer_df = skincancer_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>dxtype</th>\n",
       "      <th>loc</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id   age  sex  dxtype  loc  label\n",
       "0  HAM_0000118  ISIC_0027419  80.0    1       3    3      2\n",
       "1  HAM_0000118  ISIC_0025030  80.0    1       3    3      2\n",
       "2  HAM_0002730  ISIC_0026769  80.0    1       3    3      2\n",
       "3  HAM_0002730  ISIC_0025661  80.0    1       3    3      2\n",
       "4  HAM_0001466  ISIC_0031633  75.0    1       3    3      2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_skincancer_df['dxtype'] = dxtype_feat\n",
    "le_skincancer_df['sex'] = sex_feat\n",
    "le_skincancer_df['loc'] = loc_feat\n",
    "le_skincancer_df['label'] = labels\n",
    "le_skincancer_df.drop(columns=['localization', 'dx_type', 'dx'], inplace = True)\n",
    "le_skincancer_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bygg en egen dataloader-klass för bilder och csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age       80\n",
       "sex        1\n",
       "dxtype     3\n",
       "loc        3\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_skincancer_df.iloc[0, 2:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinCancerHamCSV(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform = None):\n",
    "        \"\"\"\n",
    "            csv_file (string): \n",
    "            Path to the csv file with annotations.\n",
    "            \n",
    "            root_dir (string): \n",
    "            Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "            on a sample.\n",
    "        \"\"\"\n",
    "        self.skincancer_df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.skincancer_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.skincancer_df.iloc[idx, 1])\n",
    "        \n",
    "        img_name = img_name + \".jpg\"\n",
    "        \n",
    "        image = io.imread(img_name)\n",
    "        \n",
    "        # Read columns from csv:\n",
    "        # labels:\n",
    "        skincancer_label = self.skincancer_df.iloc[idx, 6:]\n",
    "        skincancer_label_np = np.array([skincancer_label])\n",
    "        \n",
    "        # features:\n",
    "        skincancer_metadata = self.skincancer_df.iloc[idx, 2:6]\n",
    "        skincancer_metadata_np = np.array([skincancer_metadata])\n",
    "        \n",
    "        # Skapar en batch utan transforms:\n",
    "        sample = {'image': image, \\\n",
    "                  'metadata': skincancer_metadata_np, \\\n",
    "                  'label': skincancer_label_np}\n",
    "\n",
    "        # Skapar en batch med transforms:\n",
    "        if self.transform:\n",
    "            sample = {'image': self.transform(image), \\\n",
    "                      'metadata': skincancer_metadata_np, \\\n",
    "                      'label': skincancer_label_np}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samma klass som ovan, men läser från en \n",
    "# Pandas-df istället från en csv:\n",
    "\n",
    "class SkinCancerHamDF(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, root_dir, transform = None):\n",
    "\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.dataframe.iloc[idx, 1])\n",
    "        \n",
    "        img_name = img_name + \".jpg\"\n",
    "        \n",
    "        # sklearn-image:\n",
    "        # image = io.imread(img_name)\n",
    "        \n",
    "        # pillow:\n",
    "        image = Image.open(img_name)\n",
    "        \n",
    "        # Read columns from csv:\n",
    "        # labels:\n",
    "        skincancer_label = self.dataframe.iloc[idx, 6:]\n",
    "        skincancer_label_np = np.array([skincancer_label], dtype = int)\n",
    "        \n",
    "        # features:\n",
    "        skincancer_metadata = self.dataframe.iloc[idx, 2:6]\n",
    "        skincancer_metadata_np = np.array([skincancer_metadata], dtype = float)\n",
    "        \n",
    "        # Skapar en batch utan transforms:\n",
    "        sample = {'image': image, \\\n",
    "                  'metadata': skincancer_metadata_np, \\\n",
    "                  'label': skincancer_label_np}\n",
    "\n",
    "        # Skapar en batch med transforms:\n",
    "        if self.transform:\n",
    "            sample = {'image': self.transform(image), \\\n",
    "                      'metadata': skincancer_metadata_np, \\\n",
    "                      'label': skincancer_label_np}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testa klassen..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiera en batch-storlek:\n",
    "batchsz = 2**4\n",
    "\n",
    "# Definiera vilken augmentation som ska göras:\n",
    "train_data_transform = transforms.Compose([transforms.Resize([224, 224]),\\\n",
    "                                           transforms.ToTensor()\n",
    "                                           ])\n",
    "\n",
    "val_test_data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham10k_dataset = SkinCancerHamDF(dataframe = le_skincancer_df, \\\n",
    "                               root_dir = basePath, \\\n",
    "                               transform = train_data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[0.7333, 0.7451, 0.7529,  ..., 0.7647, 0.7647, 0.7647],\n",
       "          [0.7373, 0.7490, 0.7569,  ..., 0.7608, 0.7647, 0.7686],\n",
       "          [0.7412, 0.7529, 0.7569,  ..., 0.7608, 0.7686, 0.7686],\n",
       "          ...,\n",
       "          [0.6275, 0.6392, 0.6471,  ..., 0.7255, 0.7137, 0.7020],\n",
       "          [0.6157, 0.6235, 0.6392,  ..., 0.7255, 0.7098, 0.6941],\n",
       "          [0.6078, 0.6157, 0.6275,  ..., 0.7216, 0.7098, 0.6902]],\n",
       " \n",
       "         [[0.5882, 0.6039, 0.6118,  ..., 0.6000, 0.6078, 0.6157],\n",
       "          [0.5922, 0.6078, 0.6118,  ..., 0.6039, 0.6157, 0.6157],\n",
       "          [0.5882, 0.5961, 0.6000,  ..., 0.6039, 0.6157, 0.6118],\n",
       "          ...,\n",
       "          [0.4902, 0.4980, 0.5059,  ..., 0.6000, 0.5882, 0.5725],\n",
       "          [0.4863, 0.4902, 0.4902,  ..., 0.6039, 0.5843, 0.5686],\n",
       "          [0.4745, 0.4745, 0.4745,  ..., 0.6118, 0.5922, 0.5686]],\n",
       " \n",
       "         [[0.7529, 0.7647, 0.7765,  ..., 0.6549, 0.6588, 0.6667],\n",
       "          [0.7569, 0.7686, 0.7765,  ..., 0.6549, 0.6588, 0.6627],\n",
       "          [0.7529, 0.7569, 0.7608,  ..., 0.6510, 0.6706, 0.6549],\n",
       "          ...,\n",
       "          [0.6157, 0.6235, 0.6196,  ..., 0.6902, 0.6627, 0.6275],\n",
       "          [0.6078, 0.6078, 0.6039,  ..., 0.7098, 0.6627, 0.6235],\n",
       "          [0.5961, 0.5961, 0.5882,  ..., 0.7176, 0.6667, 0.6314]]]),\n",
       " 'metadata': array([[80.,  1.,  3.,  3.]]),\n",
       " 'label': array([[2]])}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ham10k_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ham10k_generator = DataLoader(ham10k_dataset, **params)\n",
    "\n",
    "ham10k_generator = DataLoader(ham10k_dataset, \\\n",
    "                              batch_size = 2**4, \\\n",
    "                              pin_memory = True, \\\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 2**4,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': tensor([[[[0.7333, 0.7451, 0.7529,  ..., 0.7647, 0.7647, 0.7647],\n",
       "           [0.7373, 0.7490, 0.7569,  ..., 0.7608, 0.7647, 0.7686],\n",
       "           [0.7412, 0.7529, 0.7569,  ..., 0.7608, 0.7686, 0.7686],\n",
       "           ...,\n",
       "           [0.6275, 0.6392, 0.6471,  ..., 0.7255, 0.7137, 0.7020],\n",
       "           [0.6157, 0.6235, 0.6392,  ..., 0.7255, 0.7098, 0.6941],\n",
       "           [0.6078, 0.6157, 0.6275,  ..., 0.7216, 0.7098, 0.6902]],\n",
       " \n",
       "          [[0.5882, 0.6039, 0.6118,  ..., 0.6000, 0.6078, 0.6157],\n",
       "           [0.5922, 0.6078, 0.6118,  ..., 0.6039, 0.6157, 0.6157],\n",
       "           [0.5882, 0.5961, 0.6000,  ..., 0.6039, 0.6157, 0.6118],\n",
       "           ...,\n",
       "           [0.4902, 0.4980, 0.5059,  ..., 0.6000, 0.5882, 0.5725],\n",
       "           [0.4863, 0.4902, 0.4902,  ..., 0.6039, 0.5843, 0.5686],\n",
       "           [0.4745, 0.4745, 0.4745,  ..., 0.6118, 0.5922, 0.5686]],\n",
       " \n",
       "          [[0.7529, 0.7647, 0.7765,  ..., 0.6549, 0.6588, 0.6667],\n",
       "           [0.7569, 0.7686, 0.7765,  ..., 0.6549, 0.6588, 0.6627],\n",
       "           [0.7529, 0.7569, 0.7608,  ..., 0.6510, 0.6706, 0.6549],\n",
       "           ...,\n",
       "           [0.6157, 0.6235, 0.6196,  ..., 0.6902, 0.6627, 0.6275],\n",
       "           [0.6078, 0.6078, 0.6039,  ..., 0.7098, 0.6627, 0.6235],\n",
       "           [0.5961, 0.5961, 0.5882,  ..., 0.7176, 0.6667, 0.6314]]],\n",
       " \n",
       " \n",
       "         [[[0.0941, 0.0980, 0.0980,  ..., 0.2392, 0.2353, 0.2392],\n",
       "           [0.0941, 0.0902, 0.0902,  ..., 0.2392, 0.2392, 0.2392],\n",
       "           [0.0902, 0.0863, 0.0863,  ..., 0.2353, 0.2353, 0.2353],\n",
       "           ...,\n",
       "           [0.2392, 0.2510, 0.2549,  ..., 0.0980, 0.0941, 0.0941],\n",
       "           [0.2392, 0.2431, 0.2510,  ..., 0.0941, 0.0941, 0.0863],\n",
       "           [0.2431, 0.2471, 0.2510,  ..., 0.0980, 0.0902, 0.0902]],\n",
       " \n",
       "          [[0.0510, 0.0510, 0.0549,  ..., 0.1451, 0.1451, 0.1490],\n",
       "           [0.0471, 0.0510, 0.0510,  ..., 0.1490, 0.1412, 0.1451],\n",
       "           [0.0510, 0.0510, 0.0510,  ..., 0.1490, 0.1412, 0.1412],\n",
       "           ...,\n",
       "           [0.1451, 0.1569, 0.1608,  ..., 0.0549, 0.0510, 0.0471],\n",
       "           [0.1490, 0.1529, 0.1569,  ..., 0.0588, 0.0510, 0.0471],\n",
       "           [0.1451, 0.1490, 0.1529,  ..., 0.0549, 0.0549, 0.0471]],\n",
       " \n",
       "          [[0.0863, 0.0902, 0.0902,  ..., 0.2471, 0.2510, 0.2510],\n",
       "           [0.0863, 0.0902, 0.0863,  ..., 0.2549, 0.2549, 0.2549],\n",
       "           [0.0863, 0.0824, 0.0863,  ..., 0.2549, 0.2549, 0.2510],\n",
       "           ...,\n",
       "           [0.2353, 0.2431, 0.2510,  ..., 0.1020, 0.0980, 0.0980],\n",
       "           [0.2353, 0.2392, 0.2431,  ..., 0.0941, 0.0941, 0.0824],\n",
       "           [0.2392, 0.2431, 0.2392,  ..., 0.0941, 0.0902, 0.0902]]],\n",
       " \n",
       " \n",
       "         [[[0.7294, 0.7373, 0.7490,  ..., 0.5137, 0.4941, 0.4863],\n",
       "           [0.7294, 0.7294, 0.7412,  ..., 0.5216, 0.5059, 0.4902],\n",
       "           [0.7098, 0.7176, 0.7255,  ..., 0.5333, 0.5176, 0.4941],\n",
       "           ...,\n",
       "           [0.7490, 0.7451, 0.7373,  ..., 0.4784, 0.4392, 0.4314],\n",
       "           [0.7373, 0.7294, 0.7255,  ..., 0.4706, 0.4353, 0.4235],\n",
       "           [0.7294, 0.7294, 0.7255,  ..., 0.4510, 0.4118, 0.3961]],\n",
       " \n",
       "          [[0.4980, 0.5059, 0.5255,  ..., 0.3686, 0.3451, 0.3255],\n",
       "           [0.4980, 0.5020, 0.5176,  ..., 0.3765, 0.3569, 0.3333],\n",
       "           [0.4902, 0.4902, 0.5020,  ..., 0.3765, 0.3608, 0.3412],\n",
       "           ...,\n",
       "           [0.5216, 0.5137, 0.4980,  ..., 0.3647, 0.3216, 0.3020],\n",
       "           [0.5137, 0.5020, 0.4902,  ..., 0.3333, 0.2902, 0.2667],\n",
       "           [0.5059, 0.5020, 0.4902,  ..., 0.2941, 0.2549, 0.2353]],\n",
       " \n",
       "          [[0.5294, 0.5412, 0.5725,  ..., 0.4275, 0.4039, 0.3922],\n",
       "           [0.5333, 0.5373, 0.5647,  ..., 0.4314, 0.4118, 0.4000],\n",
       "           [0.5333, 0.5333, 0.5490,  ..., 0.4392, 0.4157, 0.4000],\n",
       "           ...,\n",
       "           [0.5216, 0.5216, 0.5137,  ..., 0.3804, 0.3412, 0.3255],\n",
       "           [0.5059, 0.5137, 0.5137,  ..., 0.3451, 0.3020, 0.2824],\n",
       "           [0.4980, 0.5255, 0.5216,  ..., 0.3059, 0.2627, 0.2392]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.5882, 0.5922, 0.5922,  ..., 0.6353, 0.6392, 0.6431],\n",
       "           [0.5922, 0.5882, 0.5961,  ..., 0.6471, 0.6431, 0.6431],\n",
       "           [0.5961, 0.5961, 0.6000,  ..., 0.6471, 0.6431, 0.6431],\n",
       "           ...,\n",
       "           [0.5843, 0.5961, 0.6078,  ..., 0.6196, 0.6157, 0.6078],\n",
       "           [0.5725, 0.5922, 0.6039,  ..., 0.6196, 0.6157, 0.6078],\n",
       "           [0.5686, 0.5882, 0.6039,  ..., 0.6235, 0.6196, 0.6039]],\n",
       " \n",
       "          [[0.5020, 0.4980, 0.4941,  ..., 0.5529, 0.5451, 0.5412],\n",
       "           [0.5059, 0.5020, 0.4980,  ..., 0.5529, 0.5490, 0.5451],\n",
       "           [0.5098, 0.5098, 0.5020,  ..., 0.5529, 0.5529, 0.5490],\n",
       "           ...,\n",
       "           [0.5020, 0.5137, 0.5216,  ..., 0.5294, 0.5255, 0.5216],\n",
       "           [0.4941, 0.5059, 0.5176,  ..., 0.5294, 0.5294, 0.5216],\n",
       "           [0.4902, 0.5020, 0.5176,  ..., 0.5294, 0.5294, 0.5176]],\n",
       " \n",
       "          [[0.5412, 0.5451, 0.5451,  ..., 0.6314, 0.6275, 0.6275],\n",
       "           [0.5451, 0.5412, 0.5451,  ..., 0.6353, 0.6314, 0.6314],\n",
       "           [0.5490, 0.5490, 0.5451,  ..., 0.6392, 0.6314, 0.6314],\n",
       "           ...,\n",
       "           [0.5490, 0.5608, 0.5725,  ..., 0.5882, 0.5804, 0.5686],\n",
       "           [0.5412, 0.5569, 0.5686,  ..., 0.5882, 0.5804, 0.5725],\n",
       "           [0.5373, 0.5529, 0.5686,  ..., 0.5922, 0.5843, 0.5686]]],\n",
       " \n",
       " \n",
       "         [[[0.7490, 0.7569, 0.7569,  ..., 0.7451, 0.7451, 0.7412],\n",
       "           [0.7490, 0.7569, 0.7569,  ..., 0.7490, 0.7490, 0.7490],\n",
       "           [0.7608, 0.7569, 0.7569,  ..., 0.7529, 0.7529, 0.7529],\n",
       "           ...,\n",
       "           [0.7294, 0.7412, 0.7412,  ..., 0.7569, 0.7647, 0.7608],\n",
       "           [0.7333, 0.7451, 0.7451,  ..., 0.7569, 0.7608, 0.7608],\n",
       "           [0.7373, 0.7490, 0.7451,  ..., 0.7608, 0.7569, 0.7569]],\n",
       " \n",
       "          [[0.6392, 0.6471, 0.6471,  ..., 0.6235, 0.6118, 0.6078],\n",
       "           [0.6392, 0.6431, 0.6353,  ..., 0.6157, 0.6118, 0.6118],\n",
       "           [0.6431, 0.6431, 0.6314,  ..., 0.6235, 0.6157, 0.6196],\n",
       "           ...,\n",
       "           [0.6196, 0.6314, 0.6353,  ..., 0.6471, 0.6510, 0.6471],\n",
       "           [0.6235, 0.6353, 0.6353,  ..., 0.6471, 0.6510, 0.6471],\n",
       "           [0.6275, 0.6392, 0.6314,  ..., 0.6549, 0.6549, 0.6471]],\n",
       " \n",
       "          [[0.7216, 0.7451, 0.7569,  ..., 0.7373, 0.7333, 0.7294],\n",
       "           [0.7294, 0.7451, 0.7490,  ..., 0.7333, 0.7333, 0.7294],\n",
       "           [0.7412, 0.7451, 0.7412,  ..., 0.7412, 0.7373, 0.7294],\n",
       "           ...,\n",
       "           [0.7098, 0.7176, 0.7137,  ..., 0.7451, 0.7451, 0.7333],\n",
       "           [0.7137, 0.7255, 0.7216,  ..., 0.7451, 0.7451, 0.7373],\n",
       "           [0.7216, 0.7294, 0.7216,  ..., 0.7490, 0.7451, 0.7373]]],\n",
       " \n",
       " \n",
       "         [[[0.7843, 0.7725, 0.7765,  ..., 0.8196, 0.8196, 0.8118],\n",
       "           [0.7804, 0.7725, 0.7725,  ..., 0.8196, 0.8118, 0.8157],\n",
       "           [0.7333, 0.7490, 0.7608,  ..., 0.8196, 0.8196, 0.8275],\n",
       "           ...,\n",
       "           [0.7765, 0.7843, 0.7843,  ..., 0.8392, 0.8392, 0.8353],\n",
       "           [0.7804, 0.7843, 0.7843,  ..., 0.8471, 0.8471, 0.8471],\n",
       "           [0.7843, 0.7843, 0.7843,  ..., 0.8471, 0.8510, 0.8510]],\n",
       " \n",
       "          [[0.6275, 0.6235, 0.6118,  ..., 0.7176, 0.7176, 0.7098],\n",
       "           [0.6353, 0.6314, 0.6196,  ..., 0.7216, 0.7098, 0.7059],\n",
       "           [0.6000, 0.6118, 0.6157,  ..., 0.7216, 0.7137, 0.7098],\n",
       "           ...,\n",
       "           [0.6353, 0.6314, 0.6275,  ..., 0.7098, 0.7059, 0.7059],\n",
       "           [0.6471, 0.6353, 0.6275,  ..., 0.7216, 0.7176, 0.7176],\n",
       "           [0.6471, 0.6353, 0.6314,  ..., 0.7294, 0.7216, 0.7216]],\n",
       " \n",
       "          [[0.7059, 0.6980, 0.6980,  ..., 0.8157, 0.8118, 0.7961],\n",
       "           [0.7098, 0.7059, 0.7020,  ..., 0.8196, 0.8078, 0.7961],\n",
       "           [0.6863, 0.6941, 0.6980,  ..., 0.8196, 0.8078, 0.8039],\n",
       "           ...,\n",
       "           [0.7373, 0.7373, 0.7294,  ..., 0.8118, 0.8039, 0.8039],\n",
       "           [0.7451, 0.7451, 0.7373,  ..., 0.8235, 0.8157, 0.8118],\n",
       "           [0.7529, 0.7451, 0.7412,  ..., 0.8235, 0.8196, 0.8157]]]]),\n",
       " 'metadata': tensor([[[80.,  1.,  3.,  3.]],\n",
       " \n",
       "         [[80.,  1.,  3.,  3.]],\n",
       " \n",
       "         [[80.,  1.,  3.,  3.]],\n",
       " \n",
       "         [[80.,  1.,  3.,  3.]],\n",
       " \n",
       "         [[75.,  1.,  3.,  3.]],\n",
       " \n",
       "         [[75.,  1.,  3.,  3.]],\n",
       " \n",
       "         [[60.,  1.,  3.,  3.]],\n",
       " \n",
       "         [[60.,  1.,  3.,  3.]],\n",
       " \n",
       "         [[70.,  0.,  3.,  3.]],\n",
       " \n",
       "         [[70.,  0.,  3.,  3.]],\n",
       " \n",
       "         [[55.,  0.,  3.,  3.]],\n",
       " \n",
       "         [[85.,  0.,  3.,  3.]],\n",
       " \n",
       "         [[85.,  0.,  3.,  3.]],\n",
       " \n",
       "         [[70.,  1.,  3.,  3.]],\n",
       " \n",
       "         [[70.,  1.,  3.,  3.]],\n",
       " \n",
       "         [[65.,  1.,  3.,  3.]]], dtype=torch.float64),\n",
       " 'label': tensor([[[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]],\n",
       " \n",
       "         [[2]]])}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ham10k_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create code for moving to GPU, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De båda nedanstående blocken används för att enkelt flytta till GPU: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flytta train_loader och test_loader till GPU:n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(ham10k_generator, device)\n",
    "#valid_dl = DeviceDataLoader(valid_loader, device)\n",
    "#test_dl = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dl.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dl.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bygg ett eget CNN-nät:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 5, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 64, kernel_size = 5, stride = 2)\n",
    "        self.fc1 = nn.Linear(in_features = 64*13*13, out_features = 100)\n",
    "        self.fc2 = nn.Linear(in_features = 100, out_features = 7)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        # X = F.max_pool2d(X, kernel_size=2, stride=2)\n",
    "        m = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        X = m(X)\n",
    "        X = F.relu(self.conv2(X)) # 12  - 5 + 1 = 8px\n",
    "        #\n",
    "        # Kan man slå isär dessa steg? Japp!\n",
    "        # X = self.conv2(X) # 12  - 5 + 1 = 8px\n",
    "        # X = F.relu(X)\n",
    "        #\n",
    "        X = F.max_pool2d(X, kernel_size = 2, stride = 2) # 8/2 = 4px\n",
    "        # print(X.shape) # kommer visa sig när vi tränar modellen nedan...\n",
    "        X = X.view(-1, 64*13*13) # flatten\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        return F.log_softmax(X, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(17)\n",
    "model = ConvolutionalNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antalet träningsbara parametrar är = 1109287 st.\n"
     ]
    }
   ],
   "source": [
    "trainableparameters = []\n",
    "for param in model.parameters():\n",
    "    # trainableparameters = param.numel()\n",
    "    trainableparameters.append(param.numel())\n",
    "\n",
    "print(\"Antalet träningsbara parametrar är =\", np.sum(trainableparameters), \"st.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hur blir tensor-storlekarna när de flödar genom ovanstående nät?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {'image': torch.tensor([1,2,3]), 'label': torch.tensor([1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ursprunglig storlek:  torch.Size([16, 3, 224, 298])\n"
     ]
    }
   ],
   "source": [
    "for x in ham10k_generator:\n",
    "    x = x['image']\n",
    "    print(\"Ursprunglig storlek: \", x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efter Conv2d(1, 3, 5, 2):  torch.Size([16, 16, 110, 147])\n"
     ]
    }
   ],
   "source": [
    "x = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 5, stride = 2)(x)\n",
    "print(\"Efter Conv2d(1, 3, 5, 2): \", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efter Pooling, kernel_size = 2, stride = 2 så blir storleken:  torch.Size([16, 16, 55, 73])\n"
     ]
    }
   ],
   "source": [
    "x = nn.MaxPool2d(kernel_size = 2, stride = 2)(x)\n",
    "print(\"Efter Pooling, kernel_size = 2, stride = 2 så blir storleken: \", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 26, 35])\n"
     ]
    }
   ],
   "source": [
    "x = nn.Conv2d(in_channels = 16, out_channels = 64, kernel_size = 5, stride = 2)(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efter Pooling, kernel_size = 2, stride = 2 så blir storleken:  torch.Size([16, 64, 13, 17])\n"
     ]
    }
   ],
   "source": [
    "x = nn.MaxPool2d(kernel_size = 2, stride = 2)(x)\n",
    "print(\"Efter Pooling, kernel_size = 2, stride = 2 så blir storleken: \", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 14144])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(-1, 64*13*17).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flytta modellen till GPU, om en sådan finns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalNetwork(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (conv2): Conv2d(16, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (fc1): Linear(in_features=10816, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flytta modellen till GPU:\n",
    "\n",
    "to_device(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifiera att modellen är på rätt device:\n",
    "# True => modellen finns på GPU.\n",
    "\n",
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define loss function & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiera loss-function och vilken optimerare som ska användas:\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainingloop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10015"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ham10k_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[2]]])\n"
     ]
    }
   ],
   "source": [
    "for X in ham10k_generator:\n",
    "    print(X['label'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 298])\n",
      "torch.Size([16, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for inputs in ham10k_generator:\n",
    "        images = inputs['image']\n",
    "        labels = inputs['label']\n",
    "        \n",
    "        print(images.shape)\n",
    "        print(labels.shape)\n",
    "        #predictions = model.forward(images)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-a1ed96c25406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-38723914dd90>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# X = F.max_pool2d(X, kernel_size=2, stride=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_general/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_general/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_general/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_training_time = time.time()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "no_training_images = len(ham10k_dataset)\n",
    "\n",
    "train_accuracy = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_epoch_time = time.time()\n",
    "    correct_pred_per_epoch = 0\n",
    "    \n",
    "    for inputs in ham10k_generator:\n",
    "        images = inputs['image']\n",
    "        labels = inputs['label']\n",
    "        \n",
    "        predictions = model.forward(images)\n",
    "        train_loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Beräkna statistik från träningen på träningsmängden:\n",
    "        predicted = torch.max(predictions.data, 1)[1]\n",
    "        is_correct = (predicted == labels).sum()\n",
    "        correct_pred_per_epoch += is_correct\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # För träningsmängden:\n",
    "    # Lagra accuracy och loss per epok för t.ex. plottning:\n",
    "    accuracy_per_epoch = correct_pred_per_epoch.item() / no_training_images\n",
    "    train_accuracy.append(accuracy_per_epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    end_epoch_time = time.time()\n",
    "    epoch_time = end_epoch_time - start_epoch_time\n",
    "    # epoch startar på 0, därav \"+1\" nedan:\n",
    "    print(f\"Epok {epoch+1:02}, {epoch_time:2.2f} sek.: Träning: accuracy = {accuracy_per_epoch:4.4f} och loss är = {train_loss:4.4f}\")\n",
    "    \n",
    "end_training_time = time.time()\n",
    "\n",
    "delta = end_training_time - start_training_time\n",
    "\n",
    "print(f'\\nTraining took {delta/60:.2f} minutes.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gammal loop för backup...\n",
    "\n",
    "\n",
    "import time\n",
    "start_training_time = time.time()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "no_training_images = len(ham10k_dataset)\n",
    "\n",
    "train_accuracy = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_epoch_time = time.time()\n",
    "    train_correct_pred_per_epoch = 0\n",
    "    \n",
    "    for inputs in ham10k_generator:\n",
    "        images = input['image']\n",
    "        labels = input['labels']\n",
    "        predictions = model.forward(images)\n",
    "        train_loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Beräkna statistik från träningen på träningsmängden:\n",
    "        train_predicted = torch.max(y_train_pred.data, 1)[1]\n",
    "        train_is_correct = (train_predicted == y_train).sum()\n",
    "        train_correct_pred_per_epoch += train_is_correct\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # För träningsmängden:\n",
    "    # Lagra accuracy och loss per epok för t.ex. plottning:\n",
    "    train_epoch_accuracy = train_correct_pred_per_epoch.item() / no_training_images\n",
    "    train_accuracy.append(train_epoch_accuracy)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    end_epoch_time = time.time()\n",
    "    epoch_time = end_epoch_time - start_epoch_time\n",
    "    # epoch startar på 0, därav \"+1\" nedan:\n",
    "    print(f\"Epok {epoch+1:02}, {epoch_time:2.2f} sek.: Träning: accuracy = {train_epoch_accuracy:4.4f} och loss är = {train_loss:4.4f}\")\n",
    "    \n",
    "end_training_time = time.time()\n",
    "\n",
    "delta = end_training_time - start_training_time\n",
    "\n",
    "print(f'\\nTraining took {delta/60:.2f} minutes.')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valideringsmängden:\n",
    "model.eval()\n",
    "\n",
    "valid_correct_pred_per_epoch = 0\n",
    "valid_accuracy = []\n",
    "valid_losses = []\n",
    "\n",
    "no_valid_images = len(valid_loader.dataset) # = 998\n",
    "with torch.no_grad():\n",
    "    # Antal iterationer = 998 / 16 = 63 st.\n",
    "    for X_valid, y_valid in valid_dl:\n",
    "\n",
    "        y_valid_pred = model.forward(X_valid)\n",
    "        \n",
    "        valid_predicted = torch.max(y_valid_pred.data, 1)[1]\n",
    "        valid_is_correct = (valid_predicted == y_valid).sum()\n",
    "        valid_correct_pred_per_epoch += valid_is_correct\n",
    "        \n",
    "        valid_loss = criterion(y_valid_pred, y_valid)\n",
    "        valid_losses.append(valid_loss.item())\n",
    "            \n",
    "        valid_epoch_accuracy = valid_correct_pred_per_epoch.item() / no_valid_images\n",
    "        valid_accuracy.append(valid_epoch_accuracy)\n",
    "        \n",
    "# epoch startar på 0, därav \"+1\" nedan:\n",
    "print(f\"Validering: accuracy = {valid_epoch_accuracy:4.4f} och loss är = {valid_loss:4.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotta accuracy och loss:\n",
    "\n",
    "max_loss = np.max(train_losses).item()\n",
    "max_accuracy = np.max(train_accuracy).item()\n",
    "y_max = 1.1*max(max_loss, max_accuracy)\n",
    "\n",
    "plt.plot(range(epochs), train_accuracy)\n",
    "plt.plot(range(epochs), train_losses)\n",
    "plt.title(\"Accuracy and loss per epoch on the train set\")\n",
    "plt.legend(['Accuracy', 'Loss'], loc='lower left')\n",
    "plt.yticks(np.arange(0, y_max, step = 0.2))\n",
    "plt.ylim(0, y_max)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_loss = np.max(valid_losses).item()\n",
    "max_accuracy = np.max(valid_accuracy).item()\n",
    "y_max = 1.1*max(max_loss, max_accuracy)\n",
    "\n",
    "plt.plot(range(len(valid_losses)), valid_accuracy)\n",
    "plt.plot(range(len(valid_losses)), valid_losses)\n",
    "plt.title(\"Accuracy and loss on the validation set\")\n",
    "plt.legend(['Accuracy', 'Loss'], loc='upper left')\n",
    "plt.yticks(np.arange(0, y_max, step = 0.2))\n",
    "plt.ylim(0, y_max)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode = was_training)\n",
    "                    return\n",
    "        model.train(mode = was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elaborate training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 16\n",
    "\n",
    "no_training_images = len(dataset_trainloader.dataset) # 7007\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    \n",
    "    # Run the training batches\n",
    "    for currentbatch, (X_train, y_train) in enumerate(train_dl):\n",
    "        currentbatch += 1\n",
    "        \n",
    "        # Apply the model\n",
    "        y_pred_train = model(X_train)  # we don't flatten X-train here\n",
    "        loss = criterion(y_pred_train, y_train)\n",
    " \n",
    "        # Tally the number of correct predictions\n",
    "        predicted = torch.max(y_pred_train.data, 1)[1]\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_corr += batch_corr\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print interim results\n",
    "        if currentbatch % 100 == 0:\n",
    "            print(f'epoch: {epoch:2}  batch: {currentbatch:4} [{batchsz*currentbatch:4}/{no_training_images}]\\\n",
    "            loss: {loss.item():5.4f}\\\n",
    "            accuracy: {trn_corr.item()*100/(batchsz*currentbatch):5.1f} %')\n",
    "        \n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "\n",
    "print(f'\\nTraining took: {time.time() - start_time:.0f} seconds.') # print the time elapsed    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluate the model on the test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "no_test_images = len(test_loader.dataset)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    # Antal iterationer = 2010 / 16 = 123 st.\n",
    "    for X_test, y_test in test_dl:\n",
    "        y_pred_test = model.forward(X_test)\n",
    "        predicted = torch.max(input = y_pred_test, dim = 1)[1]\n",
    "        correct += (predicted == y_test).sum()\n",
    "        \n",
    "print(f'Test accuracy: {correct.item()}/{no_test_images} = {correct.item()*100/(no_test_images):5.2f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_general",
   "language": "python",
   "name": "pytorch_general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
