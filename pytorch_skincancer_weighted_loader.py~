#!/usr/bin/env python
# coding: utf-8

# Skincancer HAM-dataset med Pytorch/mobilenet_v2

do_training = False

# Standardimporter

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
from torchvision import datasets, transforms, models
from torchvision.utils import make_grid
import torch.optim as optim
from torch.optim import lr_scheduler

import time
import os

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

# Visa om CPU eller GPU finns tillgänglig:

def get_default_device():
    """Pick GPU if available, else CPU"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')


device = get_default_device()


print(device)


# De båda nedanstående blocken används för att enkelt flytta till GPU: 

def to_device(data, device):
    """Move tensor(s) to chosen device"""
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

class DeviceDataLoader():
    """Wrap a dataloader to move data to a device"""
    def __init__(self, dl, device):
        self.dl = dl
        self.device = device
        
    def __iter__(self):
        """Yield a batch of data after moving it to device"""
        for b in self.dl: 
            yield to_device(b, self.device)

    def __len__(self):
        """Number of batches"""
        return len(self.dl)


if torch.cuda.is_available():
    print('gpu')
else:
    print('cpu')


# Definiera en modell:

# Nedanstående två rader måste göras oavsett träning eller ej:

model = models.mobilenet_v2()
model.classifier = nn.Linear(in_features = 1280, out_features = 7, bias = True)

if do_training == True:
    
    # Vi måste ändra på det sista FC-lagret i modellen, som från början
    # innehållet 1000 st out-features. Vi behöver bara 7.

    for param in model.parameters():
        param.require_grad = False
    
    # Ersätt sista fc-lagret med rätt antal ut-klasser: 
    model.classifier = nn.Linear(in_features = 1280, out_features = 7, bias = True)
    
else:
    if torch.cuda.is_available():
        model.load_state_dict(torch.load('trained_models/mobilenet_v2_e128_b32_lre4.pt'))
    else:
        model.load_state_dict(torch.load('trained_models/mobilenet_v2_e128_b32_lre4.pt', map_location=torch.device('cpu')))
    model.eval()

# Flytta modellen till rätt device:

to_device(model, device)


# Verifiera att modellen är på rätt device:
# True => modellen finns på GPU.

next(model.parameters()).is_cuda


# Definiera de olika mängderna för träning, validering och test:

img_w = 224
img_h = 224
img_dim = 3


# Definiera vilken augmentation som ska göras:
train_data_transform = transforms.Compose([
        transforms.RandomResizedCrop(img_w),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

val_test_data_transform = transforms.Compose([
        transforms.RandomResizedCrop(img_w),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])

basepath = "../../../ml/Datasets/skin-cancer-mnist-ham10000/images_per_label_splitted_sets/"

# Definiera de tre dataseten:
skincancer_train_dataset = datasets.ImageFolder(root = basepath + 'train/',
                                           transform = train_data_transform)

skincancer_valid_dataset = datasets.ImageFolder(root = basepath + 'val/',
                                           transform = val_test_data_transform)

skincancer_test_dataset = datasets.ImageFolder(root = basepath + 'test/',
                                              transform = val_test_data_transform)


batchsz_train = 2**4
batchsz_val = 2**4
batchsz_test = 2**4

train_num_workers = 4
test_val_num_workers = 4

# Skapa loaders för de tre dataseten:
train_loader = torch.utils.data.DataLoader(skincancer_train_dataset,
                                             batch_size = batchsz_train, 
                                             shuffle = True,
                                             pin_memory = True,
                                             drop_last = True,
                                             num_workers = train_num_workers)

valid_loader = torch.utils.data.DataLoader(skincancer_valid_dataset,
                                             batch_size = batchsz_val, 
                                             shuffle = True,
                                             num_workers = test_val_num_workers)

test_loader = torch.utils.data.DataLoader(skincancer_test_dataset,
                                             batch_size = batchsz_test, 
                                             shuffle = False,
                                             num_workers = test_val_num_workers)


# ### Skapa en weighted loader som hanterar obalansen mellan klasserna:

train_targets = train_loader.dataset.targets
#print(len(train_targets))

from torch.utils.data import WeightedRandomSampler

# Test för att oversampla vissa klasser...
# https://discuss.pytorch.org/t/how-to-implement-oversampling-in-cifar-10/16964/6

train_targets = train_loader.dataset.targets
class_count = np.unique(train_targets, return_counts=True)[1]

# Testa att köra med 1 / sevenones för att se en obalanserad, vanlig, loader
# och med 1 / class_count för att se hur det balanserade resultatet blir:

# sevenones = np.ones(7)
# weight = 1 / sevenones
weight = 1. / class_count
samples_weight = weight[train_targets]
samples_weight = torch.from_numpy(samples_weight)

# Replacement = True ger dragning med återläggning, vilket vi ska ha, 
# annars kommer de mindre klasserna "ta slut" i dragningen:
sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)

# Skapa en ny dataloader baserad på 
# träningssetet som är viktad klassivs:

# "pin_memory=True" gör att man flyttar data till GPU:n medan 
# vi kör träningen, vilket påskyndar arbetet radikalt, ffa
# om vi har komplexa modeller.
train_loader_weighted = DataLoader(skincancer_train_dataset, 
                             batch_size = batchsz_train, 
                             sampler = sampler, # kan ej ha shuffle = True med sampler!
                             pin_memory=True,
                             drop_last=True,
                             num_workers = train_num_workers)


# ### Flytta *_loader till rätt device

train_dl = DeviceDataLoader(train_loader, device)
train_dl_weighted = DeviceDataLoader(train_loader_weighted, device)
valid_dl = DeviceDataLoader(valid_loader, device)
test_dl = DeviceDataLoader(test_loader, device)

torch.manual_seed(17)

# ## Definiera loss-function och vilken metod för optimering som ska användas:

# Definiera loss-function och vilken optimerare som ska användas:

learning_rate = 1e-4
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

# Decay LR by a factor of 0.1 every 3 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 3, gamma = 0.1)


# ## Följande återställer modellens vikter mellan olika körningar

def weights_init(m):
    if isinstance(m, nn.Conv2d):
        torch.nn.init.xavier_uniform_(m.weight.data)


# # Träna och utvärdera modellen

def train_eval(epochs = 10, do_training = True, do_validation = True, training_dataloader = train_dl_weighted, validation_dataloader = valid_dl):

    # if ((do_training == False) and (do_validation == True)):
    #    print("Kan inte validera en modell som inte tränats.")
    # else:
    
    if do_training == True:
        start_training_time = time.time()

        # Placeholders för att mäta modellen:
        #global train_accuracy, train_losses, val_accuracy, val_losses
        
        train_accuracy = []
        train_losses = []
        
        val_accuracy = []
        val_losses = []

        for epoch in range(epochs):
            start_epoch_time = time.time()
            train_correct_pred_per_epoch = 0
        
            current_train_loss = 0.0
            current_train_corrects = 0
        
            # Träning ###########################################################
            # Sätt modellen i träningsläge:
            start_train_time = time.time()
            model.train()
        
            for inputs_train, labels_train in training_dataloader:
                train_predictions = model.forward(inputs_train)
                optimizer.zero_grad()
            
                train_loss = criterion(train_predictions, labels_train)
                train_loss.backward()
                
                optimizer.step()
                    
                #exp_lr_scheduler.step()
                #lr = exp_lr_scheduler.get_lr()
                
                # Nedan ger den mest troliga klassen:
                _, train_predicted = torch.max(train_predictions, 1)
    
                current_train_loss += train_loss.item() * batchsz_train
                current_train_corrects += torch.sum(train_predicted == labels_train.data)
        
            end_train_time = time.time()
        
            # Validering ########################################################
            # Sätt modellen i utvärderingsläge:
            
            if do_validation == True:
                start_eval_time = time.time()
                model.eval()
        
                current_val_loss = 0.0
                current_val_corrects = 0
        
                with torch.no_grad():
                
                    inputs_val, labels_val = next(iter(validation_dataloader))
                    val_predictions = model.forward(inputs_val)
                    val_loss = criterion(val_predictions, labels_val)
                    _, val_predicted = torch.max(val_predictions, 1)
                    
                    current_val_loss += val_loss.item() * batchsz_val
                    current_val_corrects += torch.sum(val_predicted == labels_val.data)
        
                end_eval_time = time.time()
            else:
                pass
            
            ######################################################################
        
            # Metrics ############################################################
            # Räkna ut acc och loss per epok:
            epoch_train_loss = np.float64(current_train_loss / num_train_images)
            epoch_train_acc = np.float64(current_train_corrects.double() / num_train_images)
        
            if do_validation == True:
                epoch_val_loss = np.float64(current_val_loss / batchsz_val)
                epoch_val_acc = np.float64(current_val_corrects.double() / batchsz_val)
            else:
                pass
                
            # Lagra accuracy och loss per epok i en lista för t.ex. plottning:
            train_losses.append(epoch_train_loss)
            train_accuracy.append(epoch_train_acc)
            
            if do_validation == True:
                val_losses.append(epoch_val_loss)
                val_accuracy.append(epoch_val_acc)
            else:
                pass
                
            # Räkna ut tiderna per epok:
            end_epoch_time = time.time()
            epoch_time = end_epoch_time - start_epoch_time
        
            # epoch startar på 0, därav "+1" nedan:
            if do_validation == True:
                print(f"Epok {epoch + 1:02}: {epoch_time:2.1f} sek, train-acc = {epoch_train_acc:4.3f}, val-acc = {epoch_val_acc:4.3f}, train-loss = {epoch_train_loss:4.4f}, val-loss = {epoch_val_loss:4.4f}")
            else:
                print(f"Epok {epoch + 1:02}: {epoch_time:2.1f} sek, train-acc = {epoch_train_acc:4.3f}, train-loss = {epoch_train_loss:4.4f}")
    
        end_training_time = time.time()
    
        delta = end_training_time - start_training_time
    
        print(f'\nTraining took {delta:.2f} seconds.')
        
        # Definiera ett filnamn för att spara och ladda modellen:
        modelname = "MobileNetV2"
        filename = modelname + "_e" + str(epochs) + "_b" +str(batchsz_train) + "_lr" + str(f'{learning_rate:.0e}')
        
        # Spara modellen:
        # https://pytorch.org/docs/master/notes/serialization.html
        if do_training == True:
            model_save_folder = "trained_models"
            model_name = "MobileNetV2"
            model_file_suffix = ".pt"
            model_filename = model_name + "_e" + str(epochs) + "_b" +str(batchsz_train) +             "_lr" + str(f'{learning_rate:.0e}')

            full_model_filename = model_save_folder + "/" + model_filename + model_file_suffix

            torch.save(model.state_dict(), full_model_filename)
        else:
            pass
        
        # Spara training-history (acc/loss) om vi tränat 
        # modellen:
        log_save_folder = "logs"
        log_filename = model_filename
        log_file_suffix = ".csv"
        full_log_filename = log_save_folder + "/" + log_filename + log_file_suffix
        
        global training_log
        
        # Speciell range nedan för att starta på epok 1 och ej 0:
        training_log = pd.DataFrame(data={"epoch": range(1, epochs + 1),                                               "train_acc": train_accuracy,                                               "train_loss": train_losses,                                                "val_acc": val_accuracy,                                               "val_loss": val_losses})
        training_log.to_csv(full_log_filename, sep=',',index = False)


if do_training == False:
    model_save_folder = "trained_models"
    model_name = "MobileNetV2"
    model_file_suffix = ".pt"
    model_filename = model_name + "_e" + str(400) + "_b" +str(4) + "_lr" + "e-5"

    full_model_filename = model_save_folder + "/" + model_filename + model_file_suffix

    log_save_folder = "logs"
    log_filename = model_filename
    log_file_suffix = ".csv"
    full_log_filename = log_save_folder + "/" + log_filename + log_file_suffix
    
    training_log = pd.read_csv(full_log_filename)
else:
    pass


# Nollställer vikterna i modellen:
model.apply(weights_init)

# Nedan för att vi använder "droplast = True"
num_train_images = batchsz_train * np.floor_divide(len(train_loader.dataset), batchsz_train)
   
# Validation:
num_val_images = batchsz_val * np.floor_divide(len(valid_loader.dataset), batchsz_val)

# Här görs själva träningen, valideringen, sparande av modellen och träningsloggen.
# Alternativt så laddas en redan färdig modell/träningslogg.
epochs = 20
train_eval(epochs, do_training=True, do_validation=True, training_dataloader=train_dl_weighted, validation_dataloader=valid_dl)


# plt.plot(training_log['epoch'], training_log['train_loss'])
# plt.title("Loss on the training set over the epochs")
# plt.yticks(np.arange(0, 50, step = 5))
# plt.ylim(0,50)
# plt.show()

# Plotta accuracy över valideringsdatat:
# plt.plot(training_log['epoch'], training_log['val_loss'])
# plt.title("Training and validation loss")
# plt.yticks(np.arange(0, 1.1, step = 0.2))
# plt.ylim(0, max())
# plt.legend(['Train loss', 'Validation loss'], loc = 'upper right')
# plt.show()

# plt.plot(training_log['epoch'], training_log['train_acc'])
# plt.title("Loss on the training set over the epochs")
# plt.yticks(np.arange(0, 50, step = 5))
# plt.ylim(0,50)
# plt.show()

# Plotta accuracy över valideringsdatat:
# plt.plot(training_log['epoch'], training_log['val_acc'])
# plt.title("Training and validation accuracy")
# plt.yticks(np.arange(0, 1.1, step = 0.2))
# plt.ylim(0, max())
# plt.legend(['Train acc', 'Validation acc'], loc = 'lower right')
# plt.show()


# Utvärdera modellen på validation- och test-set:

def evaluate_model(num_eval_images, data_loader, model):

    model.eval()

    start_eval_test_time = time.time()
    
    # Nedan för att vi inte ska uppdatera 
    # modellens vikter:

    with torch.no_grad():
        correct = 0
        # Antal iterationer = num_test_images / batchsz_test = x st.
    
        for X_test, y_test in data_loader:
            y_pred_test = model.forward(X_test)
            predicted = torch.max(input = y_pred_test, dim = 1)[1]
            correct += (predicted == y_test).sum()
    
        end_eval_test_time = time.time()
    
        eval_test_time = end_eval_test_time - start_eval_test_time

    print(f'Test accuracy: {correct.item()}/{num_eval_images} = {correct.item()*100/(num_eval_images):5.2f} %')
    print(f"\nEvaluation took {eval_test_time:.2f} seconds.")


num_test_images = len(test_loader.dataset)

evaluate_model(num_test_images, test_dl, model)

num_valid_images = len(valid_loader.dataset)

evaluate_model(num_valid_images, valid_dl, model)


# Confusion matrix

number_of_classes = len(test_loader.dataset.classes)

def pytorch_confusion_matrix(num_classes, model, dataloader):
    
    confusion_matrix = torch.zeros(num_classes, num_classes)
    
    with torch.no_grad():
        for i, (inputs, classes) in enumerate(dataloader):
            inputs = inputs.to(device)
            classes = classes.to(device)
            
            predictions = model.forward(inputs)
            _, preds = torch.max(predictions, 1)
            
            for t, p in zip(classes.view(-1), preds.view(-1)):
                confusion_matrix[t.long(), p.long()] += 1
    
    return confusion_matrix

# Beräkningen på test-setet tar ca. 24 sek.
# på den stationära datorn med GPU med 4 workers, 
# och ca. 19 sek. med 8 workers.

start_cm_time = time.time()
cm_test = pytorch_confusion_matrix(number_of_classes, model, test_dl)

end_cm_time = time.time()

delta_cm_time = end_cm_time - start_cm_time

print(f'Beräkningen av CM tog {delta_cm_time:.2f}.')

# Spara confusion matrix till en fil...

torch.save(cm_test, 'results/conf_mat_test_set_model_e400_b4.pt')

# Ladda CM från en fil:
cm_test = torch.load('results/conf_mat_test_set_model_e400_b4.pt')

cm_test_np = cm_test.numpy()

# Beräkna första radens summa:
sum_row_one = np.sum(cm_test_np[:,0])

# Då blir t.ex. precision för label 1:
precision_label_one = cm_test_np[0][0].item() / sum_row_one


print(f'Precision för label 1 = {precision_label_one:.3f}')


# Motsvarande blir recall för label 1:

sum_col_1 = np.sum(cm_test_np[0,:])
recall_label_one = cm_test_np[0][0] / sum_col_1

print(f'Recall för label 1 = {recall_label_one:.3f}')

# Få ut labels så här:
test_loader.dataset.classes[0]

# Skapa en sammanställning över precision/recall för alla sju klasser.

# Precision:
print("Precision för respektive klass:")
for i in range(number_of_classes):
    sum_row_i = np.sum(cm_test_np[:, i])
    recall_i = cm_test_np[i,i] / sum_row_i
    print(f'{test_loader.dataset.classes[i]:5}: {recall_i:.3f}')


# Recall:
print("\nRecall för respektive klass:")
for j in range(number_of_classes):
    sum_col_j = np.sum(cm_test_np[j, :])
    recall_j = cm_test_np[j,j] / sum_col_j
    print(f'{test_loader.dataset.classes[j]:5}: {recall_j:.3f}')
